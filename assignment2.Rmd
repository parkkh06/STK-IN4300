---
title: "STK4300 - Assignment 2"
author: "Kyunhee Park"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(dplyr)
library(caret)
library(purrr)
library(ggplot2)
library(MASS)
library(glmnet)
library(boot)
library(mgcv)
library(rpart)
library(rpart.plot)
library(gt)
library(mlbench)
library(class)
library(caret)
library(ipred)
library(randomForest)
library(reshape2)
```

# Problem 1. Regression

Data load and preprocessing
```{r 1-data_load}
data_raw_q1 <- read.csv2("data/qsar_aquatic_toxicity.csv", header = FALSE)
colnames(data_raw_q1) <- c("TPSA", "SAacc", "H050", "MLOGP", "RDCHI", "GATS1p", "nN", "C040", "LC50")

data_raw_q1 <- data_raw_q1 %>% 
  dplyr::mutate_all(as.numeric)
```


## 1-(a) Plain linear regression.

Train-test set split
```{r 1-a}
set.seed(123)

sample <- sample(c(TRUE, FALSE), nrow(data_raw_q1), replace=TRUE, prob=c(2/3, 1/3))
train_data  <- data_raw_q1[sample, ]
test_data   <- data_raw_q1[!sample, ]
```

### 1-(a)-(i) Regression - linear effect.

(i) model each of them directly as a linear effect
```{r 1-a-i}
linear_reg_mdl_linear <- lm(LC50 ~ TPSA + SAacc + H050 + MLOGP + RDCHI + GATS1p + nN + C040,
                            data = train_data)

train_pred_linear <- predict(linear_reg_mdl_linear, train_data)
test_pred_linear <- predict(linear_reg_mdl_linear, test_data)

# Performance metrics: MSE
train_error_linear <- mean((train_pred_linear - train_data$LC50)^2)
test_error_linear <- mean((test_pred_linear - test_data$LC50)^2)

summary(linear_reg_mdl_linear)
```

### 1-(a)-(ii) Regression - Dummy encoding.

(ii) transform each of them using a 0/1 dummy encoding where 0 represents absence of the specific atom and 1 represents presence of the specific atoms.

**Data encoding**
```{r 1-a-ii-encode}
train_data_encode <- train_data %>% 
  dplyr::mutate(H050_encode = ifelse(H050 > 0, 1, 0),
                nN_encode = ifelse(nN > 0, 1, 0),
                C040_encode = ifelse(C040 > 0, 1, 0)) %>%
  dplyr::select(-H050, -nN, -C040)

test_data_encode <- test_data %>%
  dplyr::mutate(H050_encode = ifelse(H050 > 0, 1, 0),
                nN_encode = ifelse(nN > 0, 1, 0),
                C040_encode = ifelse(C040 > 0, 1, 0)) %>%
  dplyr::select(-H050, -nN, -C040)
```


**Model training**
```{r 1-a-ii-mdl}
linear_reg_mdl_encode <- lm(LC50 ~ TPSA + SAacc + H050_encode + MLOGP + 
                              RDCHI + GATS1p + nN_encode + C040_encode,
                            data = train_data_encode)

train_pred_encode <- predict(linear_reg_mdl_encode, train_data_encode)
test_pred_encode <- predict(linear_reg_mdl_encode, test_data_encode)

# Performance metrics: MSE
train_error_encode <- mean((train_pred_encode - train_data_encode$LC50)^2)
test_error_encode <- mean((test_pred_encode - test_data_encode$LC50)^2)

summary(linear_reg_mdl_encode)
```


- Linear effect has lower error on both Train and Test set.
- Encoding H050, nN, C040 induces information losses.


## 1-(b) Bootstrap - linear regression.

```{r 1-b}
model_training <- function(input_df) {
  
  sample <- sample(c(TRUE, FALSE), nrow(input_df), replace=TRUE, prob=c(2/3, 1/3))
  train_data  <- input_df[sample, ]
  test_data   <- input_df[!sample, ]

  train_data_encode <- train_data %>% 
  dplyr::mutate(H050_encode = ifelse(H050 > 0, 1, 0),
                nN_encode = ifelse(nN > 0, 1, 0),
                C040_encode = ifelse(C040 > 0, 1, 0)) %>%
  dplyr::select(-H050, -nN, -C040)

  test_data_encode <- test_data %>%
    dplyr::mutate(H050_encode = ifelse(H050 > 0, 1, 0),
                  nN_encode = ifelse(nN > 0, 1, 0),
                  C040_encode = ifelse(C040 > 0, 1, 0)) %>%
    dplyr::select(-H050, -nN, -C040)

  mdl_linear <- lm(LC50 ~ TPSA + SAacc + H050 + MLOGP + RDCHI + GATS1p + nN + C040, 
                   data = train_data)

  train_pred_linear <- predict(mdl_linear, train_data)
  test_pred_linear <- predict(mdl_linear, test_data)

  train_error_linear <- mean((train_pred_linear - train_data$LC50)^2)
  test_error_linear <- mean((test_pred_linear - test_data$LC50)^2)


  mdl_encode <- lm(LC50 ~ TPSA + SAacc + H050_encode + MLOGP + RDCHI + 
                     GATS1p + nN_encode + C040_encode,
                   data = train_data_encode)

  train_pred_encode <- predict(mdl_encode, train_data_encode)
  test_pred_encode <- predict(mdl_encode, test_data_encode)

  train_error_encode <- mean((train_pred_encode - train_data_encode$LC50)^2)
  test_error_encode <- mean((test_pred_encode - test_data_encode$LC50)^2)
  
  return(
    list(
      train_error_linear = train_error_linear,
      test_error_linear = test_error_linear,
      train_error_encode = train_error_encode,
      test_error_encode = test_error_encode)
    )
}

bootstrap_train_test_error_df <- purrr::map_dfr(1:200, ~ model_training(data_raw_q1))

bootstrap_avg_test_error_linear <- mean(bootstrap_train_test_error_df$test_error_linear)
bootstrap_avg_test_error_encode <- mean(bootstrap_train_test_error_df$test_error_encode)

ggplot(bootstrap_train_test_error_df) +
  geom_histogram(aes(x = test_error_linear, fill = "Test Error - Linear"), 
                 binwidth = 0.05, alpha = 0.5) +
  geom_histogram(aes(x = test_error_encode, fill = "Test Error - Encode"), 
                 binwidth = 0.05, alpha = 0.5) +
  geom_vline(aes(xintercept = bootstrap_avg_test_error_linear, 
                 color = "Average Test Error - Linear"), 
             linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = bootstrap_avg_test_error_encode, 
                 color = "Average Test Error - Encode"), 
             linetype = "dashed", size = 1) +
  scale_fill_manual(
    name = "Test Error Distribution",
    values = c("Test Error - Linear" = "blue", "Test Error - Encode" = "green")
  ) +
  scale_color_manual(
    name = "Average Test Error",
    values = c("Average Test Error - Linear" = "black", "Average Test Error - Encode" = "red")
  ) +
  labs(
    title = "Distribution of Test Errors",
    x = "Test Error",
    y = "Count"
  ) +
  theme_minimal()
```

- We reduce the influence of any particular train-test split that could have been unusually favorable or unfavorable for either model.
- This allows us to observe the average performance and variability of each model under different data conditions, providing a more stable insight into each model's effectiveness.
- Dummy encoding in Option (ii) can lead to worse performance because it reduces the granularity of information for count variables (like H050, nN, C040).
- When we replace continuous or count data with binary indicators, we lose information about the magnitude of these variables. For instance, whether a molecule has one nitrogen atom or five becomes indistinguishable once we binarize it. This simplification can lead to a loss in predictive power, as the actual count values may carry nuanced information relevant to toxicity predictions. Thus, models using dummy encoding often struggle to capture relationships that might be effectively captured by the original count values, leading to higher test errors.


## 1-(c) Variable selection.

**Variable selection**
- Forward selection and Backward elimination
- AIC, BIC
```{r 1-c-variable-selection}
full_model <- lm(LC50 ~ TPSA + SAacc + H050 + MLOGP + RDCHI + GATS1p + nN + C040, 
                 data = train_data)

null_model <- lm(LC50 ~ 1, 
                 data = train_data)

# Backward, AIC
backward_aic <- step(full_model, direction = "backward", k = 2)

# Backward, BIC
backward_bic <- step(full_model, direction = "backward", k = log(nrow(train_data)))

# Forward, AIC
forward_aic <- step(null_model, scope = formula(full_model), direction = "forward", k = 2)

# Forward, BIC
forward_bic <- step(null_model, scope = formula(full_model), 
                    direction = "forward", k = log(nrow(train_data)))

selected_backward_aic <- names(coef(backward_aic))[-1]
selected_backward_bic <- names(coef(backward_bic))[-1]
selected_forward_aic <- names(coef(forward_aic))[-1]
selected_forward_bic <- names(coef(forward_bic))[-1]
```


```{r 1-c-result}
cat("Backward Selection (AIC):", selected_backward_aic, "\n")
cat("Backward Selection (BIC):", selected_backward_bic, "\n")
cat("Forward Selection (AIC):", selected_forward_aic, "\n")
cat("Forward Selection (BIC):", selected_forward_bic, "\n")
```

- The variable selection shows the same result for Backward- and Forward selection on both AIC and BIC criteria.

```{r 1-c-mdl-training}
variable_selected_formula <- as.formula(paste("LC50 ~", 
                                          paste(selected_forward_aic, collapse = " + ")))

variable_selected_mdl <- lm(variable_selected_formula, 
                            data = test_data)

variable_selected_mdl_test_error <- mean((variable_selected_mdl$fitted.values - test_data$LC50)^2)

cat("Test error (Variable selection) :", variable_selected_mdl_test_error, "\n")

```



## 1-(d) Apply ridge regression and use both a bootstrap procedure.


```{r 1-d}

X <- as.matrix(train_data %>% dplyr::select(-LC50)) 
y <- train_data$LC50

# Lambda grid
lambda_grid <- 10^seq(3, -3, length = 100)

# CV - optimal lamnda
set.seed(123)
cv_ridge <- cv.glmnet(X, y, alpha = 0, lambda = lambda_grid, nfolds = 5)
optimal_lambda_cv <- cv_ridge$lambda.min

# Bootstrap to find optimal lambda
bootstrap_ridge <- function(data, indices) {
  # Bootstrap resampling
  X_boot <- X[indices, ]
  y_boot <- y[indices]

  # Use cross-validation on bootstrap sample to find the best lambda
  cv_fit <- cv.glmnet(X_boot, y_boot, alpha = 0, lambda = lambda_grid, nfolds = 5)
  return(cv_fit$lambda.min)  
}

# Bootstraping
bootstrap_results <- boot(data = train_data, statistic = bootstrap_ridge, R = 300)
optimal_lambda_boot <- mean(bootstrap_results$t)  # Average lambda from bootstrap

lambda_results <- data.frame(Lambda = bootstrap_results$t)

ggplot(lambda_results, aes(x = Lambda)) +
  geom_histogram(aes(fill = "Histogram of Lambda - Bootstrap"), bins = 15, position = "identity") +
  geom_vline(aes(xintercept = optimal_lambda_cv, color = "Optimal Lambda CV"), 
             linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = optimal_lambda_boot, color = "Optimal Lambda Bootstrap"), 
             linetype = "dashed", size = 1) +
  labs(
    title = "Optimal Lambda Selection by Cross-Validation and Bootstrap",
    x = "Lambda Value",
    y = "Frequency"
  ) +
    scale_fill_manual(
    name = "Distribution",
    values = c("Histogram of Lambda - Bootstrap" = "lightblue")
  ) +
  scale_color_manual(
    name = "Optimal Lambda",
    values = c("Optimal Lambda CV" = "blue", "Optimal Lambda Bootstrap" = "red")
  ) +
  theme_minimal()


```

- Bootstrap repeatedly samples the data, which includes variability.
- Bootstrap can capture more variance and provide a conservative estimate that emphasizes stability.
- CV method provides a reliable estimate and is less computationally intensive than bootstrap.
- Using both approaches provides insight into the stability and robustness of the model under different sampling conditions. 
- Assist in selecting a complexity parameter $\lambda$ that generalizes well to unseen data.


## 1-(e) Generalised additive model (GAM).

```{r 1-e}
set.seed(123)

gam_mdl_low_comp <- mgcv::gam(LC50 ~ s(TPSA, k = 3) + s(SAacc, k = 3) +
                                        s(H050, k = 3) + s(MLOGP, k = 3) +
                                        s(RDCHI, k = 3) + s(GATS1p, k = 3) +
                                        s(nN, k = 3) + s(C040, k = 3),
                              data = train_data)

gam_mdl_high_comp <- mgcv::gam(LC50 ~ s(TPSA, k = 10) + s(SAacc, k = 10) +
                                         s(H050, k = 10) + s(MLOGP, k = 10) +
                                         s(RDCHI, k = 10) + s(GATS1p, k = 10) +
                                         s(nN, k = 9) + s(C040, k = 5),
                               data = train_data)

summary(gam_mdl_low_comp)
summary(gam_mdl_high_comp)
```

```{r 1-e-result}
gam_mdl_test_pred_low <- predict(gam_mdl_low_comp, newdata = test_data)
gam_mdl_test_pred_high <- predict(gam_mdl_high_comp, newdata = test_data)

gam_mdl_test_error_low <- mean((gam_mdl_test_pred_low - test_data$LC50)^2)
gam_mdl_test_error_high <- mean((gam_mdl_test_pred_high - test_data$LC50)^2)

cat("Test Error (Low Complexity):", gam_mdl_test_error_low, "\n")
cat("Test Error (High Complexity):", gam_mdl_test_error_high, "\n")
```

- Low complexity shows lower test error.
- Low complexity GAM model avoids overfitting.


## 1-(f) Regression Tree
```{r 1-f}
# Fit the regression tree
reg_tree_mdl <- rpart(LC50 ~ TPSA + SAacc + H050 + MLOGP + RDCHI + GATS1p + nN + C040,
                    data = train_data,
                    method = "anova")

rpart.plot(reg_tree_mdl, main = "Regression Tree for LC50")

printcp(reg_tree_mdl)
plotcp(reg_tree_mdl)

# Pruning
optimal_cp <- reg_tree_mdl$cptable[which.min(reg_tree_mdl$cptable[, "xerror"]), "CP"]
reg_tree_mdl_pruned <- prune(reg_tree_mdl, cp = optimal_cp)

# pruned tree
rpart.plot(reg_tree_mdl_pruned, main = "Pruned Regression Tree for LC50")

# test data
test_pred_reg_tree <- predict(reg_tree_mdl_pruned, newdata = test_data)

test_error_mse_reg_tree <- mean((test_pred_reg_tree - test_data$LC50)^2)

```

- Original tree size: 12
- Pruned tree size: 11

```{r 1-f-result}
cat("Mean Squared Error (MSE):", test_error_mse_reg_tree, "\n")
```


## 1-(g) Model comparison.
```{r 1-g}
results_table <- data.frame(
  Model = c(
    "1-(a)-i - Linear effect",
    "1-(a)-ii - Dummy encoded",
    "1-(b) - Boostrap - Linear effect",
    "1-(b) - Boostrap - Dummy encoded",
    "1-(c) - Variable selection",
    "1-(e) - GAM - Low Complexity",
    "1-(e) - GAM - High Complexity",
    "1-(f) - Regression Tree"
  ),
  Test_Error = c(
    test_error_linear,
    test_error_encode,
    bootstrap_avg_test_error_linear,
    bootstrap_avg_test_error_encode,
    variable_selected_mdl_test_error,
    gam_mdl_test_error_low,
    gam_mdl_test_error_high,
    test_error_mse_reg_tree
  )
)

# Create the gt table
results_table %>%
  gt() %>%
  tab_header(
    title = "Model Comparison - Test Errors"
  ) %>%
  cols_label(
    Model = "Model Type",
    Test_Error = "Test Error (MSE)"
  ) %>%
  fmt(
    columns = vars(Test_Error), 
    fns = function(x) round(x, 4)  
  )

```

- Variable selection shows the lowest test error. Avoided overfitting.
- Highest test error for Regression tree. Possible sing of overfitting.
- Model training with Linear effect shows low test error in general. Dummy encoding has lost necessary information for model training.
- For GAM model, low complexity shows lower test error. Avoided overfitting. However, high complexity indicates there might have been overfitting due to its high complexity.


# Problem 2. Classification

```{r 2, include=FALSE}
data("PimaIndiansDiabetes2", package = "mlbench")
df <- na.omit(PimaIndiansDiabetes2)
# train-test split
set.seed(42)
train_index <- createDataPartition(df$diabetes, p = 2/3, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]
```

## 2-(a) k-NN
```{r 2-a}
# scaling
train_scaled <- train_data %>%
  mutate(across(where(is.numeric), scale))
test_scaled <- test_data %>%
  mutate(across(where(is.numeric), scale))

# Define range of k values to test
k_values <- 1:20

ctrl_cv_5_fold <- trainControl(method = "cv", number = 5)
ctrl_loocv <- trainControl(method = "LOOCV")

cv_error_5_fold <- function(k) {
  knn_model <- train(diabetes ~ ., 
                     data = train_scaled,
                     method = "knn",
                     tuneGrid = data.frame(k = k),
                     trControl = ctrl_cv_5_fold)
  1 - max(knn_model$results$Accuracy)
}

cv_error_loocv <- function(k) {
  knn_model <- train(diabetes ~ ., data = train_scaled,
                     method = "knn",
                     tuneGrid = data.frame(k = k),
                     trControl = ctrl_loocv)
  1 - max(knn_model$results$Accuracy)
}

test_error <- function(k) {
  test_pred <- knn(train_scaled[,-9], test_scaled[,-9], train_scaled$diabetes, k = k)
  mean(test_pred != test_scaled$diabetes)
}

cv_errors_5fold <- purrr::map_dbl(k_values, cv_error_5_fold)
cv_errors_loocv <- purrr::map_dbl(k_values, cv_error_loocv)
test_errors <- purrr::map_dbl(k_values, test_error)

error_df <- data.frame(k = k_values,
                       `5-Fold CV Error` = cv_errors_5fold,
                       `LOOCV Error` = cv_errors_loocv,
                       `Test Error` = test_errors)

error_df_long <- reshape2::melt(error_df,
                                id.vars = "k", 
                                variable.name = "Error_Type", 
                                value.name = "Error")

ggplot(error_df_long, aes(x = k, y = Error, color = Error_Type)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(title = "k-NN Classification Errors for Different k Values",
       x = "Number of Neighbors (k)",
       y = "Error Rate") +
  scale_color_manual(values = c("blue", "red", "black")) +
  theme_minimal() +
  theme(legend.title = element_blank())

knn_test_error <- error_df_long %>%
  filter(grepl("Test", Error_Type)) %>%  
  summarise(minimum_error = min(Error, na.rm = TRUE))

cat("Test error - KNN: ", knn_test_error$minimum_error)

```

- Test error is lowest when k = 6, 7, 9, 10. Optimal k should be one of these numbers.
- LOOCV error and CV error decrease as the number of neighbors increase.
- However, test error increases as number of k increases.
- After k = 12, the model shows the indication of underfitting.


## 2-(b) Generalized additive model

```{r 2-b-gam-full}
# Variable selection - backward selection, AIC
gam_full <- gam(diabetes ~ s(pregnant) + s(glucose) + s(pressure) + s(triceps) +
                      s(insulin) + s(mass) + s(pedigree) + s(age),
                    family = binomial, data = train_data, select = TRUE)

summary(gam_full)

gam_full_pred <- predict(gam_full, newdata = test_data, type = "response")
test_data$predicted <- ifelse(gam_full_pred > 0.5, 'pos', 'neg')
test_error <- mean(test_data$predicted != test_data$diabetes)
cat("Test error - Full GAM: ", test_error)

```

```{r 2-b-gam-selected}
gam_selected <- gam(diabetes ~ s(glucose) + s(age) + s(mass) + s(pedigree),
                    family = binomial, data = train_data)

gam_selected_pred <- predict(gam_selected, newdata = test_data, type = "response")
test_data$predicted <- ifelse(gam_selected_pred > 0.5, 'pos', 'neg')

gam_selected_test_error <- mean(test_data$predicted != test_data$diabetes)
cat("Test error - selected GAM: ", gam_selected_test_error)

```

- Test error of the selected GAM model has increased.
- Selected GAM model uses 4 features while full GAM model uses 8 features.

### 2-(b) - Significant predictors
- p-value larger than 0.05
- glucose: EDF = 1, linear effects on the response variable.
- mass: EDF = 2.013, higher non-linear effects on the response variable than glucose.
- pedigree: EDF = 1.083, linear effects on the response variable.
- age: EDF = 1.832, higher non-linear effects on the response variable.


## 2-(c) Decision Tree

### 2-(c)-i Classification tree
```{r 2-c-i}
# Classification Tree
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class")
tree_pred_train <- predict(tree_model, train_data, type = "class")
tree_pred_test <- predict(tree_model, test_data, type = "class")

tree_train_error <- mean(tree_pred_train != train_data$diabetes)
tree_test_error <- mean(tree_pred_test != test_data$diabetes)
```


### 2-(c)-ii Ensemble of bagged tree
```{r 2-c-ii}
# Bagged Trees
bagged_model <- bagging(diabetes ~ ., data = train_data, coob = TRUE)
bagged_pred_train <- predict(bagged_model, train_data)
bagged_pred_test <- predict(bagged_model, test_data)

bagged_train_error <- mean(bagged_pred_train != train_data$diabetes)
bagged_test_error <- mean(bagged_pred_test != test_data$diabetes)
```

### 2-(c)-iii Random forest
```{r 2-c-iii}
# Random Forest
rf_model <- randomForest(diabetes ~ ., data = train_data)
rf_pred_train <- predict(rf_model, train_data)
rf_pred_test <- predict(rf_model, test_data)

rf_train_error <- mean(rf_pred_train != train_data$diabetes)
rf_test_error <- mean(rf_pred_test != test_data$diabetes)
```

### 2-(c)-result
```{r 2-c-result}
results_df <- data.frame(
  Model = c("Classification Tree", "Bagged Trees", "Random Forest"),
  Training_Error = c(tree_train_error, bagged_train_error, rf_train_error),
  Test_Error = c(tree_test_error, bagged_test_error, rf_test_error)
)

results_table <- results_df %>%
  gt() %>%
  fmt_number(columns = c("Training_Error", "Test_Error"), decimals = 4) %>%
  tab_header(
    title = "Training and Test Errors",
  ) %>%
  cols_label(
    Model = "Model",
    Training_Error = "Training Error",
    Test_Error = "Test Error"
  ) %>%
  tab_style(
    style = cell_fill(color = "lightblue"),
    locations = cells_column_labels()
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels()
  )

results_table
```

#### 2-(c)-result-Classification Tree:

- Highest test error.
- Simple and can capture non-linear relationships in the data. 
- Possibility of overfitting the training data, a higher test error.

#### 2-(c)-result-Bagged Trees:

- Lowest test error
- Reduces the variance of the model by averaging multiple decision trees trained on different bootstrap samples of the training data. 
- Lower test error compared to a single classification tree.

#### 2-(c)-result-Random Forest:

- Introduces randomness in the feature selection process.
- Decorrelates the trees and reduces overfitting. 

## 2-(d) Neural Network

```{r 2-d}
# scaling
train_scaled <- train_data %>%
  mutate(across(where(is.numeric), scale))
test_scaled <- test_data %>%
  mutate(across(where(is.numeric), scale))

ctrl <- trainControl(method = "cv", number = 5)

# grid search - size of NN, regularization
grid <- expand.grid(size = seq(1, 20, by = 2), decay = c(0, 0.1, 0.5, 1))

set.seed(123)
nn_model <- train(diabetes ~ ., 
                  data = train_scaled, 
                  method = "nnet",
                  tuneGrid = grid,
                  trControl = ctrl,
                  linout = FALSE,         
                  trace = FALSE,          
                  maxit = 200)            

# best nn model
print(nn_model$bestTune)

nn_train_pred <- predict(nn_model, newdata = train_scaled)
nn_test_pred <- predict(nn_model, newdata = test_scaled)

nn_train_error <- mean(nn_train_pred != train_data$diabetes)
nn_test_error <- mean(nn_test_pred != test_data$diabetes)

cat("Training Error: ", nn_train_error, "\n")
cat("Test Error: ", nn_test_error, "\n")

ggplot(nn_model) + 
  labs(title = "Neural Network Model Tuning",
       x = "Number of Neurons in Hidden Layer (size)",
       y = "Accuracy") +
  theme_minimal()

```

- grid search is implemented: size from 1-20, decay factor(0, 0.1, 0.5, 1)
- Best NN model: size = 7, decay = 0.5


## 2-(e) Model comparison
```{r 2-e}
results_table <- data.frame(
  Model = c(
    "KNN",
    "GAM",
    "Classification Tree",
    "Bagged Tree",
    "Random Forest",
    "Neural Network"
  ),
  Test_Error = c(
    knn_test_error$minimum_error,
    gam_selected_test_error,
    tree_test_error,
    bagged_test_error,
    rf_test_error,
    nn_test_error
  )
)

# Create the gt table
results_table %>%
  gt() %>%
  tab_header(
    title = "Model Comparison - Test Errors"
  ) %>%
  cols_label(
    Model = "Model Type",
    Test_Error = "Test Error"
  ) %>%
  fmt(
    columns = vars(Test_Error),  # Use vars() to specify the columns
    fns = function(x) round(x, 4)  # Round to 4 decimal places
  )
```

- Final model selection: **GAM**
- GAM has the lowest test error.
- Feature selection has been conducted in GAM.
- Able to assess the features' impact on the response variable.
- GAM is highly interpretable.
- Random forest has the second lowest test error.
- Neural Network does not seem to be efficient considering its computational burdensome.
