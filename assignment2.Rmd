---
title: "assignment_2"
author: "Kyunhee Park"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(dplyr)
library(caret)
library(purrr)
library(ggplot2)
library(MASS)
library(glmnet)
library(boot)
library(mgcv)
library(rpart)
library(rpart.plot)
library(gt)
library(mlbench)
library(class)
library(caret)
library(ipred)
library(randomForest)
```

# Problem 1. Regression

Data load and preprocessing
```{r 1-data_load}
data_raw_q1 <- read.csv2("data/qsar_aquatic_toxicity.csv", header = FALSE)
colnames(data_raw_q1) <- c("TPSA", "SAacc", "H050", "MLOGP", "RDCHI", "GATS1p", "nN", "C040", "LC50")

data_raw_q1 <- data_raw_q1 %>% 
  dplyr::mutate_all(as.numeric)
```


## 1-(a) Plain linear regression.

Train-test set split
```{r 1-a}
set.seed(123)

sample <- sample(c(TRUE, FALSE), nrow(data_raw_q1), replace=TRUE, prob=c(2/3, 1/3))
train_data  <- data_raw_q1[sample, ]
test_data   <- data_raw_q1[!sample, ]
```

### 1-(a)-(i) Regression - linear effect.

(i) model each of them directly as a linear effect
```{r 1-a-i}
linear_reg_mdl_linear <- lm(LC50 ~ TPSA + SAacc + H050 + MLOGP + RDCHI + GATS1p + nN + C040,
                            data = train_data)

train_pred_linear <- predict(linear_reg_mdl_linear, train_data)
test_pred_linear <- predict(linear_reg_mdl_linear, test_data)

# Performance metrics: MSE
train_error_linear <- mean((train_pred_linear - train_data$LC50)^2)
test_error_linear <- mean((test_pred_linear - test_data$LC50)^2)

summary(linear_reg_mdl_linear)
```

### 1-(a)-(ii) Regression - Dummy encoding.

(ii) transform each of them using a 0/1 dummy encoding where 0 represents absence of the specific atom and 1 represents presence of the specific atoms.

**Data encoding**
```{r 1-a-ii-encode}
train_data_encode <- train_data %>% 
  dplyr::mutate(H050_encode = ifelse(H050 > 0, 1, 0),
                nN_encode = ifelse(nN > 0, 1, 0),
                C040_encode = ifelse(C040 > 0, 1, 0)) %>%
  dplyr::select(-H050, -nN, -C040)

test_data_encode <- test_data %>%
  dplyr::mutate(H050_encode = ifelse(H050 > 0, 1, 0),
                nN_encode = ifelse(nN > 0, 1, 0),
                C040_encode = ifelse(C040 > 0, 1, 0)) %>%
  dplyr::select(-H050, -nN, -C040)
```


**Model training**
```{r 1-a-ii-mdl}
linear_reg_mdl_encode <- lm(LC50 ~ TPSA + SAacc + H050_encode + MLOGP + 
                              RDCHI + GATS1p + nN_encode + C040_encode,
                            data = train_data_encode)

train_pred_encode <- predict(linear_reg_mdl_encode, train_data_encode)
test_pred_encode <- predict(linear_reg_mdl_encode, test_data_encode)

# Performance metrics: MSE
train_error_encode <- mean((train_pred_encode - train_data_encode$LC50)^2)
test_error_encode <- mean((test_pred_encode - test_data_encode$LC50)^2)

summary(linear_reg_mdl_encode)
```


- Linear effect has lower error on both Train and Test set.
- Encoding H050, nN, C040 induces information losses.


## 1-(b) Bootstrap - linear regression.

```{r 1-b}
model_training <- function(input_df) {
  
  sample <- sample(c(TRUE, FALSE), nrow(input_df), replace=TRUE, prob=c(2/3, 1/3))
  train_data  <- input_df[sample, ]
  test_data   <- input_df[!sample, ]

  train_data_encode <- train_data %>% 
  dplyr::mutate(H050_encode = ifelse(H050 > 0, 1, 0),
                nN_encode = ifelse(nN > 0, 1, 0),
                C040_encode = ifelse(C040 > 0, 1, 0)) %>%
  dplyr::select(-H050, -nN, -C040)

  test_data_encode <- test_data %>%
    dplyr::mutate(H050_encode = ifelse(H050 > 0, 1, 0),
                  nN_encode = ifelse(nN > 0, 1, 0),
                  C040_encode = ifelse(C040 > 0, 1, 0)) %>%
    dplyr::select(-H050, -nN, -C040)

  mdl_linear <- lm(LC50 ~ TPSA + SAacc + H050 + MLOGP + RDCHI + GATS1p + nN + C040, 
                   data = train_data)

  train_pred_linear <- predict(mdl_linear, train_data)
  test_pred_linear <- predict(mdl_linear, test_data)

  train_error_linear <- mean((train_pred_linear - train_data$LC50)^2)
  test_error_linear <- mean((test_pred_linear - test_data$LC50)^2)


  mdl_encode <- lm(LC50 ~ TPSA + SAacc + H050_encode + MLOGP + RDCHI + 
                     GATS1p + nN_encode + C040_encode,
                   data = train_data_encode)

  train_pred_encode <- predict(mdl_encode, train_data_encode)
  test_pred_encode <- predict(mdl_encode, test_data_encode)

  train_error_encode <- mean((train_pred_encode - train_data_encode$LC50)^2)
  test_error_encode <- mean((test_pred_encode - test_data_encode$LC50)^2)
  
  return(
    list(
      train_error_linear = train_error_linear,
      test_error_linear = test_error_linear,
      train_error_encode = train_error_encode,
      test_error_encode = test_error_encode)
    )
}

bootstrap_train_test_error_df <- purrr::map_dfr(1:200, ~ model_training(data_raw_q1))

bootstrap_avg_test_error_linear <- mean(bootstrap_train_test_error_df$test_error_linear)
bootstrap_avg_test_error_encode <- mean(bootstrap_train_test_error_df$test_error_encode)

ggplot(bootstrap_train_test_error_df) +
  geom_histogram(aes(x = test_error_linear, fill = "Test Error - Linear"), 
                 binwidth = 0.05, alpha = 0.5) +
  geom_histogram(aes(x = test_error_encode, fill = "Test Error - Encode"), 
                 binwidth = 0.05, alpha = 0.5) +
  geom_vline(aes(xintercept = bootstrap_avg_test_error_linear, 
                 color = "Average Test Error - Linear"), 
             linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = bootstrap_avg_test_error_encode, 
                 color = "Average Test Error - Encode"), 
             linetype = "dashed", size = 1) +
  scale_fill_manual(
    name = "Test Error Distribution",
    values = c("Test Error - Linear" = "blue", "Test Error - Encode" = "green")
  ) +
  scale_color_manual(
    name = "Average Test Error",
    values = c("Average Test Error - Linear" = "black", "Average Test Error - Encode" = "red")
  ) +
  labs(
    title = "Distribution of Test Errors",
    x = "Test Error",
    y = "Count"
  ) +
  theme_minimal()
```

- We reduce the influence of any particular train-test split that could have been unusually favorable or unfavorable for either model.
- This allows us to observe the average performance and variability of each model under different data conditions, providing a more stable insight into each model's effectiveness.
- Dummy encoding in Option (ii) can lead to worse performance because it reduces the granularity of information for count variables (like H050, nN, C040).
- When we replace continuous or count data with binary indicators, we lose information about the magnitude of these variables. For instance, whether a molecule has one nitrogen atom or five becomes indistinguishable once we binarize it. This simplification can lead to a loss in predictive power, as the actual count values may carry nuanced information relevant to toxicity predictions. Thus, models using dummy encoding often struggle to capture relationships that might be effectively captured by the original count values, leading to higher test errors.


## 1-(c) Variable selection.

**Variable selection**
- Forward selection and Backward elimination
- AIC, BIC
```{r 1-c-variable-selection}
full_model <- lm(LC50 ~ TPSA + SAacc + H050 + MLOGP + RDCHI + GATS1p + nN + C040, 
                 data = train_data)

null_model <- lm(LC50 ~ 1, 
                 data = train_data)

# Backward, AIC
backward_aic <- step(full_model, direction = "backward", k = 2)

# Backward, BIC
backward_bic <- step(full_model, direction = "backward", k = log(nrow(train_data)))

# Forward, AIC
forward_aic <- step(null_model, scope = formula(full_model), direction = "forward", k = 2)

# Forward, BIC
forward_bic <- step(null_model, scope = formula(full_model), 
                    direction = "forward", k = log(nrow(train_data)))

selected_backward_aic <- names(coef(backward_aic))[-1]
selected_backward_bic <- names(coef(backward_bic))[-1]
selected_forward_aic <- names(coef(forward_aic))[-1]
selected_forward_bic <- names(coef(forward_bic))[-1]
```


```{r 1-c-result}
cat("Backward Selection (AIC):", selected_backward_aic, "\n")
cat("Backward Selection (BIC):", selected_backward_bic, "\n")
cat("Forward Selection (AIC):", selected_forward_aic, "\n")
cat("Forward Selection (BIC):", selected_forward_bic, "\n")
```

- The variable selection shows the same result for Backward- and Forward selection on both AIC and BIC criteria.

```{r 1-c-mdl-training}
variable_selected_formula <- as.formula(paste("LC50 ~", 
                                          paste(selected_forward_aic, collapse = " + ")))

variable_selected_mdl <- lm(variable_selected_formula, 
                            data = test_data)

variable_selected_mdl_test_error <- mean((variable_selected_mdl$fitted.values - test_data$LC50)^2)

cat("Test error (Variable selection) :", variable_selected_mdl_test_error, "\n")

```



## 1-(d) Apply ridge regression and use both a bootstrap procedure.


```{r 1-d}

X <- as.matrix(train_data %>% dplyr::select(-LC50)) 
y <- train_data$LC50

# Lambda grid
lambda_grid <- 10^seq(3, -3, length = 100)

# CV - optimal lamnda
set.seed(123)
cv_ridge <- cv.glmnet(X, y, alpha = 0, lambda = lambda_grid, nfolds = 5)
optimal_lambda_cv <- cv_ridge$lambda.min  # Lambda with minimum MSE in CV

# Bootstrap to find optimal lambda
bootstrap_ridge <- function(data, indices) {
  # Bootstrap resampling
  X_boot <- X[indices, ]
  y_boot <- y[indices]

  # Use cross-validation on bootstrap sample to find the best lambda
  cv_fit <- cv.glmnet(X_boot, y_boot, alpha = 0, lambda = lambda_grid, nfolds = 5)
  return(cv_fit$lambda.min)  # Return lambda that minimizes MSE
}

# Bootstraping
bootstrap_results <- boot(data = train_data, statistic = bootstrap_ridge, R = 300)
optimal_lambda_boot <- mean(bootstrap_results$t)  # Average lambda from bootstrap

lambda_results <- data.frame(Lambda = bootstrap_results$t)

ggplot(lambda_results, aes(x = Lambda)) +
  geom_histogram(aes(fill = "Histogram of Lambda - Bootstrap"), bins = 15, position = "identity") +
  geom_vline(aes(xintercept = optimal_lambda_cv, color = "Optimal Lambda CV"), 
             linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = optimal_lambda_boot, color = "Optimal Lambda Bootstrap"), 
             linetype = "dashed", size = 1) +
  labs(
    title = "Optimal Lambda Selection by Cross-Validation and Bootstrap",
    x = "Lambda Value",
    y = "Frequency"
  ) +
    scale_fill_manual(
    name = "Distribution",
    values = c("Histogram of Lambda - Bootstrap" = "lightblue")
  ) +
  scale_color_manual(
    name = "Optimal Lambda",
    values = c("Optimal Lambda CV" = "blue", "Optimal Lambda Bootstrap" = "red")
  ) +
  theme_minimal()


```

- Bootstrap repeatedly samples the data, which includes variability.
- Bootstrap can capture more variance and provide a conservative estimate that emphasizes stability.
- CV method provides a reliable estimate and is less computationally intensive than bootstrap.
- Using both approaches provides insight into the stability and robustness of the model under different sampling conditions. 
- Assist in selecting a complexity parameter $\lambda$ that generalizes well to unseen data.


## 1-(e) Generalised additive model (GAM).

```{r 1-e}
set.seed(123)
#sample <- sample(c(TRUE, FALSE), nrow(data_raw_q1), replace = TRUE, prob = c(2/3, 1/3))
#train_data <- data_raw_q1[sample, ]
#test_data <- data_raw_q1[!sample, ]

gam_mdl_low_comp <- mgcv::gam(LC50 ~ s(TPSA, k = 3) + s(SAacc, k = 3) +
                                        s(H050, k = 3) + s(MLOGP, k = 3) +
                                        s(RDCHI, k = 3) + s(GATS1p, k = 3) +
                                        s(nN, k = 3) + s(C040, k = 3),
                              data = train_data)

gam_mdl_high_comp <- mgcv::gam(LC50 ~ s(TPSA, k = 10) + s(SAacc, k = 10) +
                                         s(H050, k = 10) + s(MLOGP, k = 10) +
                                         s(RDCHI, k = 10) + s(GATS1p, k = 10) +
                                         s(nN, k = 9) + s(C040, k = 5),
                               data = train_data)

summary(gam_mdl_low_comp)
summary(gam_mdl_high_comp)
```

```{r 1-e-result}
gam_mdl_test_pred_low <- predict(gam_mdl_low_comp, newdata = test_data)
gam_mdl_test_pred_high <- predict(gam_mdl_high_comp, newdata = test_data)

gam_mdl_test_error_low <- mean((gam_mdl_test_pred_low - test_data$LC50)^2)
gam_mdl_test_error_high <- mean((gam_mdl_test_pred_high - test_data$LC50)^2)

cat("Test Error (Low Complexity):", gam_mdl_test_error_low, "\n")
cat("Test Error (High Complexity):", gam_mdl_test_error_high, "\n")
```

- Low complexity shows lower test error.
- Low complexity GAM model avoids overfitting.


## 1-(f) Regression Tree
```{r 1-f}
# Fit the regression tree
reg_tree_mdl <- rpart(LC50 ~ TPSA + SAacc + H050 + MLOGP + RDCHI + GATS1p + nN + C040,
                    data = train_data,
                    method = "anova")

rpart.plot(reg_tree_mdl, main = "Regression Tree for LC50")

printcp(reg_tree_mdl)
plotcp(reg_tree_mdl)

# Pruning
optimal_cp <- reg_tree_mdl$cptable[which.min(reg_tree_mdl$cptable[, "xerror"]), "CP"]
reg_tree_mdl_pruned <- prune(reg_tree_mdl, cp = optimal_cp)

# pruned tree
rpart.plot(reg_tree_mdl_pruned, main = "Pruned Regression Tree for LC50")

# test data
test_pred_reg_tree <- predict(reg_tree_mdl_pruned, newdata = test_data)

test_error_mse_reg_tree <- mean((test_pred_reg_tree - test_data$LC50)^2)

```

- Original tree size: 12
- Pruned tree size: 11

```{r 1-f-result}
cat("Mean Squared Error (MSE):", test_error_mse_reg_tree, "\n")
```



## 1-(g) Model comparison.
```{r 1-g}
results_table <- data.frame(
  Model = c(
    "1-(a)-i - Linear effect",
    "1-(a)-ii - Dummy encoded",
    "1-(b) - Boostrap - Linear effect",
    "1-(b) - Boostrap - Dummy encoded",
    "1-(c) - Variable selection",
    "1-(e) - GAM - Low Complexity",
    "1-(e) - GAM - High Complexity",
    "1-(f) - Regression Tree"
  ),
  Test_Error = c(
    test_error_linear,
    test_error_encode,
    bootstrap_avg_test_error_linear,
    bootstrap_avg_test_error_encode,
    variable_selected_mdl_test_error,
    gam_mdl_test_error_low,
    gam_mdl_test_error_high,
    test_error_mse_reg_tree
  )
)

# Create the gt table
results_table %>%
  gt() %>%
  tab_header(
    title = "Model Comparison - Test Errors"
  ) %>%
  cols_label(
    Model = "Model Type",
    Test_Error = "Test Error (MSE)"
  ) %>%
  fmt(
    columns = vars(Test_Error),  # Use vars() to specify the columns
    fns = function(x) round(x, 4)  # Round to 4 decimal places
  )

```

- Variable selection shows the lowest test error. Avoided overfitting.
- Highest test error for Regression tree. Possible sing of overfitting.
- Model training with Linear effect shows low test error in general. Dummy encoding has lost necessary information for model training.
- For GAM model, low complexity shows lower test error. Avoided overfitting. However, high complexity indicates there might have been overfitting due to its high complexity.


# Problem 2. Classification

```{r 2, include=FALSE}
data("PimaIndiansDiabetes2", package = "mlbench")
df <- na.omit(PimaIndiansDiabetes2)  # Remove rows with NA values
```

## 2-(a) k-NN
```{r 2-a}
set.seed(42)
train_index <- createDataPartition(df$diabetes, p = 2/3, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# Standardize the features (scaling)
train_scaled <- train_data %>%
  mutate(across(where(is.numeric), scale))
test_scaled <- test_data %>%
  mutate(across(where(is.numeric), scale))

# Define range of k values to test
k_values <- 1:20

# Define 5-Fold and LOOCV cross-validation controls
ctrl_5fold <- trainControl(method = "cv", number = 5)
ctrl_loocv <- trainControl(method = "LOOCV")

# Function to calculate 5-fold CV error for a given k
cv_error_5fold <- function(k) {
  knn_model <- train(diabetes ~ ., 
                     data = train_scaled,
                     method = "knn",
                     tuneGrid = data.frame(k = k),
                     trControl = ctrl_5fold)
  1 - max(knn_model$results$Accuracy)
}

# Function to calculate LOOCV error for a given k
cv_error_loocv <- function(k) {
  knn_model <- train(diabetes ~ ., data = train_scaled,
                     method = "knn",
                     tuneGrid = data.frame(k = k),
                     trControl = ctrl_loocv)
  1 - max(knn_model$results$Accuracy)
}

# Function to calculate test error for a given k
test_error <- function(k) {
  test_pred <- knn(train_scaled[,-9], test_scaled[,-9], train_scaled$diabetes, k = k)
  mean(test_pred != test_scaled$diabetes)
}

# Apply each function over k_values using purrr::map_dbl to return a numeric vector
cv_errors_5fold <- purrr::map_dbl(k_values, cv_error_5fold)
cv_errors_loocv <- purrr::map_dbl(k_values, cv_error_loocv)
test_errors <- purrr::map_dbl(k_values, test_error)

# Combine results into a data frame for plotting
error_df <- data.frame(k = k_values,
                       `5-Fold CV Error` = cv_errors_5fold,
                       `LOOCV Error` = cv_errors_loocv,
                       `Test Error` = test_errors)

# Melt the data for easier plotting with ggplot2
library(reshape2)
error_df_long <- melt(error_df, id.vars = "k", variable.name = "Error_Type", value.name = "Error")

# Plot the errors for different values of k
ggplot(error_df_long, aes(x = k, y = Error, color = Error_Type)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(title = "k-NN Classification Errors for Different k Values",
       x = "Number of Neighbors (k)",
       y = "Error Rate") +
  scale_color_manual(values = c("blue", "red", "black")) +
  theme_minimal() +
  theme(legend.title = element_blank())

```
## 2-(b)
```{r 1-b}
gam_full <- gam(diabetes ~ s(pregnant) + s(glucose) + s(pressure) + s(triceps) +
                  s(insulin) + s(mass) + s(pedigree) + s(age),
                family = binomial, data = df)

# Summarize the full model to view terms and significance levels
summary(gam_full)

# Perform variable selection using a backward selection approach with AIC
# `select = TRUE` allows GAM to automatically drop non-significant terms based on AIC
gam_selected <- gam(diabetes ~ s(pregnant) + s(glucose) + s(pressure) + s(triceps) +
                      s(insulin) + s(mass) + s(pedigree) + s(age),
                    family = binomial, data = df, select = TRUE)

# Summarize the selected model
summary(gam_selected)

```


### 2-(b)-1. Significant predictors (glucose, mass, pedigree and age):
- If glucose has a high EDF and is significant, it means that glucose level has a non-linear effect on diabetes risk, with certain ranges having a higher or lower risk.
- If mass is significant with an EDF close to 1, it suggests that higher BMI generally increases the risk of diabetes, likely in a linear fashion.
- If age shows a non-linear effect, this could imply that certain age ranges have a higher influence on diabetes risk than others, capturing complex age-related effects.
- pedigree

### 2-(b)-2. Non-significant predictors (pregnant, triceps, pressure, insulin):
These variables may not significantly affect the response after accounting for other variables, suggesting they might not be essential predictors in the model.


## 2-(c)

### 2-(c)-i Classification tree
```{r 2-c-i}
# Classification Tree
tree_model <- rpart(diabetes ~ ., data = train_data, method = "class")
tree_pred_train <- predict(tree_model, train_data, type = "class")
tree_pred_test <- predict(tree_model, test_data, type = "class")

# Calculate training and test error for classification tree
tree_train_error <- mean(tree_pred_train != train_data$diabetes)
tree_test_error <- mean(tree_pred_test != test_data$diabetes)
```


### 2-(c)-ii Ensemble of bagged tree
```{r 2-c-ii}
# Bagged Trees
bagged_model <- bagging(diabetes ~ ., data = train_data, coob = TRUE)
bagged_pred_train <- predict(bagged_model, train_data)
bagged_pred_test <- predict(bagged_model, test_data)

# Calculate training and test error for bagged trees
bagged_train_error <- mean(bagged_pred_train != train_data$diabetes)
bagged_test_error <- mean(bagged_pred_test != test_data$diabetes)

```

### 2-(c)-iii Random forest
```{r 2-c-iii}
# Random Forest
rf_model <- randomForest(diabetes ~ ., data = train_data)
rf_pred_train <- predict(rf_model, train_data)
rf_pred_test <- predict(rf_model, test_data)

# Calculate training and test error for random forest
rf_train_error <- mean(rf_pred_train != train_data$diabetes)
rf_test_error <- mean(rf_pred_test != test_data$diabetes)


```

### 2-(c)-result
```{r 2-c-result}
# Print the results
cat("Classification Tree:\n")
cat("Training Error: ", tree_train_error, "\n")
cat("Test Error: ", tree_test_error, "\n\n")

cat("Bagged Trees:\n")
cat("Training Error: ", bagged_train_error, "\n")
cat("Test Error: ", bagged_test_error, "\n\n")

cat("Random Forest:\n")
cat("Training Error: ", rf_train_error, "\n")
cat("Test Error: ", rf_test_error, "\n")
```

Classification Tree:

The classification tree is a simple model that can capture non-linear relationships in the data. However, it might overfit the training data, leading to a higher test error compared to more robust methods like bagging or random forests.
Bagged Trees:

Bagging (Bootstrap Aggregating) reduces the variance of the model by averaging multiple decision trees trained on different bootstrap samples of the training data. This usually results in a lower test error compared to a single classification tree.
Random Forest:

Random forests further improve on bagging by introducing randomness in the feature selection process, which decorrelates the trees and reduces overfitting. This often leads to the lowest test error among the three methods.

